{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Practical 05 Coding Exercises: Robot Control</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.linalg as linalg\n",
    "import pickle\n",
    "\n",
    "from Practical05_Support.Robot import *\n",
    "from Practical05_Support.Helper import *\n",
    "from Practical05_Support.Renderer import Renderer as PenguinPiRenderer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (3 pts): Implement a Pose Controller for the PenguinPi Robot\n",
    "\n",
    "In Part 2 of practical notebook, we defined a proportional controller that brings our PenguinPi Robot to a desired location in space. However, we did not control the robot's orientation.\n",
    "\n",
    "Your task is to implement the ``MoveToGoalPoseController``, which brings our robot to a goal pose that includes a desired orientation $(x_g, y_g, \\theta_g)$. \n",
    "\n",
    "**Hints**\n",
    "- The ``Helper.py`` contains many helpful functions you may need, use them as necessary.\n",
    "- Such as ``get_distance_robot_to_goal Compute(.)``, which compute distance between two points,\n",
    "- and ``clamp_angle(.)`` which limits range of angle to the range $[-\\pi, \\pi]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoveToPoseController(object):\n",
    "    \n",
    "    \"\"\"A simple move to pose proportional controller\"\"\"\n",
    "    def __init__(self, robot=None):\n",
    "        \"\"\"\n",
    "        Initialize a new move to pose proportional controller\n",
    "        :param robot: The robotic system to controlled\n",
    "        :param K_a: A proportional gain, you could add more as necessary\n",
    "        \"\"\"\n",
    "        self.robot = robot\n",
    "        \n",
    "    def run(self, goal_position=np.zeros(3), delta_time=0.01):\n",
    "        \"\"\"\n",
    "        Run control loop until the robot reaches the goal_position\n",
    "        :param goal_position: Desired goal position\n",
    "        :returns: Tuple of arrays. A 3DxN array for the system states and a 2DxN array for control inputs\n",
    "        \"\"\"\n",
    "        \n",
    "        states = []\n",
    "        controls = []\n",
    "        initial_state = self.robot.get_state()\n",
    "        states.append(initial_state)\n",
    "        \n",
    "        stop_criteria_met = False\n",
    "        \n",
    "        #TODO 1: Add control parameters here -----------------------------------------\n",
    "        K_1 = 0\n",
    "        \n",
    "        #ENDTODO ---------------------------------------------------------------------\n",
    "        \n",
    "        #TODO 2: Compute errors such as distance towards goal position ---------------\n",
    "        \n",
    "        #ENDTODO ---------------------------------------------------------------------\n",
    "        while not stop_criteria_met: \n",
    "            \n",
    "            #TODO 3: Compute the new control input -----------------------------------\n",
    "            v_k = 0\n",
    "            w_k = 0\n",
    "            #ENDTODO -----------------------------------------------------------------\n",
    "            \n",
    "            # Apply control to robot\n",
    "            self.robot.drive(v_k, w_k, delta_time)\n",
    "            new_state = self.robot.get_state()\n",
    "            \n",
    "            # Keep track of variables for plotting\n",
    "            controls.append([v_k, w_k])\n",
    "            states.append(new_state)\n",
    "                        \n",
    "            #TODO 4: Update errors ---------------------------------------------------\n",
    "            \n",
    "            #ENDTODO -----------------------------------------------------------------\n",
    "            #TODO 5: Check for stopping criteria -------------------------------------\n",
    "            if True:\n",
    "            #ENDTODO -----------------------------------------------------------------\n",
    "                stop_criteria_met = True\n",
    "            \n",
    "                       \n",
    "        return np.array(states), np.array(controls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your Solution\n",
    "\n",
    "**Visually**: The position and orientation of your robot should match the target pose currently displayed as a fixed 2D coordinate frame at (1.5, 0.0)\n",
    "\n",
    "**Evaluation**: ``dist_travel`` and ``pose_difference`` are computed for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define robot, desired goal and delta time\n",
    "start_pose = np.array([-1.5, 0.0, -np.pi/2])\n",
    "desired_goal = np.array([1.5, 1.0, 0])\n",
    "delta_t = 0.01\n",
    "bot = PenguinPi(init_state=start_pose)\n",
    "\n",
    "# Instantiate controller and compute sequence of control signal\n",
    "controller = MoveToPoseController(bot)\n",
    "robot_states, robot_controls = controller.run(goal_position=desired_goal, delta_time=delta_t)\n",
    "\n",
    "#================== Do not change this part=======================================#\n",
    "# Evaluation of your implementation\n",
    "dist_travel = sum((np.power(robot_states[1:-1,0:2]-robot_states[0:-2,0:2],2) @ np.ones([2,1]))**0.5)\n",
    "pose_error = abs(robot_states[-1,:] - desired_goal)\n",
    "#=================================================================================#\n",
    "print('Distance travelled is: ', dist_travel)\n",
    "print('Pose error is: ', pose_error)\n",
    "\n",
    "# Define renderer and start animation\n",
    "rend = PenguinPiRenderer.Instance()\n",
    "rend.initialize(robot_states, robot_controls, target_pose=desired_goal, dt_data=delta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (3 pts): Implement a Discrete Time Finite Horizon LQR Controller\n",
    "\n",
    "The solution to the discrete-time finite horizon LQR problem is given by the following algorithm:\n",
    "\n",
    "1. Set $P_N$ = Q\n",
    "2. For $t = N,\\dots,1$\n",
    "\n",
    "<center>$P_{t-1} = Q + A^TP_tA - A^TP_tB(R+B^TP_tB)^{-1}B^TP_tA$</center>\n",
    "\n",
    "3. For $t = 0,\\dots,N-1$, define\n",
    "<center>$K_{t} = -(R+B^TP_{t+1}B)^{-1}B^TP_{t+1}A$</center>\n",
    "\n",
    "4. For $t = 0,\\dots,N-1$, optimal $\\mathbf{u}_t$ is given by <center>$\\mathbf{u}_t = K_t(\\mathbf{x}_t - \\mathbf{x}_g$)</center>\n",
    "\n",
    "You are tasked with defining the corresponding ``DiscreteFiniteLQR`` Python class. A skeleton class is provided below. \n",
    "\n",
    "**Please keep in mind**\n",
    "- ``self.P`` = $[P_1, P_2,\\dots,P_N]$. Each time you compute a new P matrix, add it to the self.P attribute (defined as a list) using the method ``self.P.append(new_P)``. All P matrices are needed for the computation of the gains.\n",
    "- ``self.K`` = $[K_0, K_1,\\dots,K_{N-1}]$. Each time you compute a new gain K, add it to the self.K attribute (defined as a list) using the method ``self.K.append(new_K)``. All K matrices are needed for the computation of the control inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteFiniteLQR(object):\n",
    "    \n",
    "    \"\"\"A simple LQR controller.\"\"\"\n",
    "\n",
    "    def __init__(self, Q=np.eye(2), R=0.3):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize a new LQR controller.\n",
    "        :param Q: State cost\n",
    "        :param R: Control cost\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Q = Q\n",
    "        self.R = R\n",
    "        \n",
    "        # List of solutions to the Riccati Equation for each iteration\n",
    "        self.P = []\n",
    "        # List of controller gains for each iteration\n",
    "        self.K = []\n",
    "        \n",
    "        \n",
    "    def solve(self, system, horizon=20):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute controller gain for a given system\n",
    "        :param system: Linear system for which a control law needs to be\n",
    "                       computed\n",
    "        :param horizon: Planning horizon\n",
    "        \"\"\"\n",
    "        \n",
    "        A = system.A\n",
    "        B = system.B\n",
    "        \n",
    "        # Set P_N = Q\n",
    "        self.P.append(self.Q)\n",
    "        \n",
    "        #TODO 1: Compute all P matrices ------------------------------------------\n",
    "        # Append each new P matrix to the class attribute self.P\n",
    "        for i in range(horizon, 0, -1):\n",
    "        \n",
    "        \tbreak # Delete break and add your code here\n",
    "        #ENDTODO -----------------------------------------------------------------\n",
    "        \n",
    "        #TODO 2: Compute all gains K  --------------------------------------------\n",
    "        # Append each new K gain to the class attribute self.K\n",
    "        for i in range(horizon):\n",
    "        \n",
    "        \tbreak # Delete break and add your code here\n",
    "         #ENDTODO ----------------------------------------------------------------\n",
    "            \n",
    "                \n",
    "    def run_control_loop(self, system, desired_state, horizon):\n",
    "        \"\"\"\n",
    "        Compute control inputs required for the system to reach the desired state\n",
    "        :param system: Linear system for which a control inputs need to be\n",
    "                       computed\n",
    "        :param desired_state: Target state for the system\n",
    "        :param horizon: Planning horizon\n",
    "        :returns: Tuple of arrays, 1Dxhorizon array for the system outputs (i.e., position) and 1Dxhorizon array for control inputs\n",
    "        \"\"\"\n",
    "         \n",
    "        u_controls, y_outputs = [], []\n",
    "\n",
    "        for i in range(horizon):\n",
    "            \n",
    "            #TODO 3: Compute control inputs -------------------------------------------\n",
    "            #Hint system would be the Robot1D model seen in practical notebook\n",
    "            # Get current state for computation of control u_t\n",
    "            x_t = 0\n",
    "            \n",
    "            # Get system's output\n",
    "            y_t = 0\n",
    "            \n",
    "            # Compute control u_t\n",
    "            u_t = 0\n",
    "            # and apply control to the robot\n",
    "            \n",
    "            #ENDTODO -----------------------------------------------------------------\n",
    "            \n",
    "            # Add values to return types\n",
    "            u_controls += [u_t]\n",
    "            y_outputs += [y_t]\n",
    "             \n",
    "        \n",
    "        return np.array(y_outputs), np.array(u_controls)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Solution\n",
    "\n",
    "Compute $\\mathbf{u}_t$ and $\\mathbf{y}_t$ using the gains obtained from your LQR controller. You can test your output against the values stored in the test files ``lqr_test.pk``.\n",
    "\n",
    "Your implementation will be compared against another set of test cases for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test case and define variables\n",
    "with open('Practical05_Support/pickle/lqr_test.pk', 'rb') as read_from:\n",
    "    test = pickle.load(read_from)\n",
    "\n",
    "# Create an instance of our system\n",
    "A=test['A']\n",
    "B=test['B']\n",
    "C=test['C']\n",
    "initial_state=test['initial_state']\n",
    "print('A', A, '\\nB', B, '\\nC', C)\n",
    "\n",
    "my_system = Robot1D(A=A, B=B, C=C, initial_state=initial_state)\n",
    "\n",
    "# Define the state and control costs as well as the planning horizon\n",
    "Q = test['Q']\n",
    "R = test['R']\n",
    "horizon = test['horizon']\n",
    "print('Q',Q, '\\nR',R, '\\nhorizon',horizon)\n",
    "\n",
    "# Create an instace of our controller and solve the LQR problem\n",
    "lqr = DiscreteFiniteLQR(Q=Q, R=R)\n",
    "lqr.solve(my_system, horizon)\n",
    "\n",
    "# Define desired state\n",
    "desired_x = test['desired_state']\n",
    "print('desired_x',desired_x)\n",
    "\n",
    "# Apply control loop\n",
    "y_array, u_array = lqr.run_control_loop(my_system, desired_x, horizon)\n",
    "\n",
    "# Plot and compare results\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.plot(np.arange(horizon), u_array, 'r', label='Your Solution')\n",
    "ax1.plot(np.arange(horizon), test['control'], 'g-.', label='Expected', lw=2)\n",
    "ax1.set_ylabel(r'$\\mathbf{u}_t$')\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "ax2.plot(np.arange(horizon), y_array, 'b', label='Your Solution')\n",
    "ax2.plot(np.arange(horizon), test['output'], 'g-.', label='Expected', lw=2)\n",
    "ax2.set_ylabel(r'$\\mathbf{y}_t$')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.legend(loc='best')\n",
    "\n",
    "ax1.set_title('LQR Finite Horizon Control Results')\n",
    "\n",
    "print('Is control input close to solution? ',np.all(np.isclose(test['control'], u_array)))\n",
    "print('Is state output close to solution? ',np.all(np.isclose(test['output'], y_array)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "## For the Penguinpi Controller:\n",
    "- Make sure that the ``run(.)`` function returns two arrays. The first array of dimensions 3xN corresponds to states. The second array of dimensions 2XN corresponds to the inputs (or controls) computed by your controller\n",
    "- Remove all print statements before submitting your solution\n",
    "- You get 2 points if ``pose_difference`` is smaller than `[0.001, 0.001, 0.23]`\n",
    "- You get the third point if ``dist_travelled`` is smaller than the example solution\n",
    "\n",
    "## For LQR exercise:\n",
    "- You will be graded based on the output of the ``run_control_loop_method(.)``. Please make sure that your solution returns the expected variables with the correct type (no rounding is needed)\n",
    "- Remove all print statements before submitting your solution\n",
    "- Discrete LQR is deterministic with a fixed Q, R, A, B and horizon, if your control signal and state matches the solution, you will get full mark which is 3 points, otherwise 0 point\n",
    "\n",
    "**NOTE**: Penguinpi Exercise might get to infinity loop if you are not careful, if there is a single block in the notebook that leads to infinity loop, you will get 0 for the whole notebook, or even if your infinity loop is dormant (living inside a function that get called during grading, you will also get 0 fro the whole notebook!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import otter\n",
    "grader = otter.Notebook(tests_dir = \"Practical05_Support/tests\")\n",
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
